{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed tensorflow version:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io, signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "print(\"Installed tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # GPU unit setting\n",
    "tf.keras.backend.floatx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Setting for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data parameters\n",
    "winT = 18000 # one priod\n",
    "\n",
    "\n",
    "### RMSprop optimizer parameters\n",
    "lr_begin = 1e-3\n",
    "RHO = 0.8\n",
    "\n",
    "### Training parameters \n",
    "MAX_EPOCHS = 80\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_1 = math.ceil(MAX_EPOCHS/2)\n",
    "EPOCH_2 = math.ceil(MAX_EPOCHS*3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load measured datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] where am I?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/wkchoi/Arc-Fault/arc_fault'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[Info.] where am I?\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define signal plot function\n",
    "def show_signal_subplot(n_show, data, label, YRNG_MIN=-1, YRNG_MAX=1):\n",
    "    plt.figure(2, figsize=(12, 8))\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    for i in range(n_show):\n",
    "        plt.subplot(5, 10, i + 1)\n",
    "        plt.plot(data[i], color='cornflowerblue')\n",
    "        plt.text(int(data.shape[1]/2), 0, \"%d\" % label[i], fontsize=12, color='r')\n",
    "        plt.xlim(0, data.shape[1])\n",
    "        plt.ylim(YRNG_MIN, YRNG_MAX)\n",
    "        \n",
    "def data_shuffle(data):\n",
    "    s = np.arange(data.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    \n",
    "    return data[s]\n",
    "    \n",
    "    \n",
    "\n",
    "def gen_one_period_data(file_path, winT):\n",
    "    \n",
    "    ##### load raw data\n",
    "    data_raw = pd.read_excel(file_path, index_col=None, header=None, sheet_name='Sheet1')\n",
    "    data = data_raw.to_numpy() # numpy array\n",
    "    \n",
    "    ##### Data Normalization\n",
    "    data_n = np.zeros((data.shape[0],data.shape[1]))\n",
    "    for i in range(data.shape[1]):\n",
    "        data_n[:,i] = data[:,i]/max(abs(data[:,i]))  \n",
    "    \n",
    "    ##### expand one-period dataset\n",
    "    dT = round(winT / 10)\n",
    "    Num = round((data_n.shape[0] - winT)/dT)\n",
    "    \n",
    "    data_tot = np.zeros((winT,Num*data_n.shape[1]))\n",
    "    index = 0\n",
    "    for i in range(data_n.shape[1]):\n",
    "        for j in range(Num):\n",
    "            index = Num*i+j\n",
    "            data_tot[:,index] = data_n[j*dT:winT+j*dT,i]\n",
    "            \n",
    "    print(\"[Info.] Total re-arranged one-period data shape: \", data_tot.shape)\n",
    "    \n",
    "    return data_tot\n",
    "\n",
    "def merge_one_period_data(winT, *file_paths):\n",
    "    \n",
    "    subArrays = []\n",
    "    for file_path in file_paths:\n",
    "        data = gen_one_period_data(file_path, winT)\n",
    "        subArrays.append(data)\n",
    "                \n",
    "    allArrays = np.concatenate(subArrays, axis=1)\n",
    "    \n",
    "    print(\"[Info.] ==========> Total merged one-period data shape: \", allArrays.shape, \"\\n\")\n",
    "        \n",
    "    return allArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/wkchoi/Arc-Fault\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total re-arranged one-period data shape:  (18000, 1480)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1560)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 80)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 600)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1880)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 7600) \n",
      "\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2160)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 8160) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Fan arc raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Fan/arc/Fan_1_arc_humid_44(c61)_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Fan/arc/Fan_2_arc_humid_45_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Fan/arc/Fan_1_arc_humid_34(c14)_c2_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Fan/arc/Fan_2_arc_humid_34(c23)_c15_data.xlsx\")\n",
    "file_path5 = os.path.join(base_dir, \"Fan/arc/fan_1_arc_data.xlsx\")\n",
    "file_path6 = os.path.join(base_dir, \"Fan/arc/fan_2_arc(c57)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4, file_path5, file_path6]\n",
    "fan_arc = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "##### Fan normal raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Fan/normal/Fan_1_normal_humid_room_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Fan/normal/Fan_2_normal_humid_50_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Fan/normal/fan_1_normal_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Fan/normal/fan_2_normal(c54)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4]\n",
    "fan_normal = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "#### Fan Data shuffle\n",
    "fan_arc_1 = data_shuffle(fan_arc.T)\n",
    "fan_arc_2 = fan_arc_1.T[:,:]\n",
    "\n",
    "fan_normal_1 = data_shuffle(fan_normal.T)\n",
    "fan_normal_2 = fan_normal_1.T[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fluor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total re-arranged one-period data shape:  (18000, 3160)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 360)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2600)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1800)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 7920) \n",
      "\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1480)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 5480) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Fluor arc raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Fluor/arc/Fluor_1_arc_humid_40(c79)_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Fluor/arc/Fluor_1_arc_humid_50(weird)_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Fluor/arc/Fluor_2_arc_humid_40(c65)_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Fluor/arc/Fluor_2_arc_humid_23(c51)_data(c45).xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4]\n",
    "fluor_arc = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "##### Fluor normal raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Fluor/normal/Fluor_1_normal_humid_room_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Fluor/normal/Fluor_2_normal_humid_34_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Fluor/normal/Fluor_2_normal(c37)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3]\n",
    "fluor_normal = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "#### Fluor Data shuffle\n",
    "fluor_arc_1 = data_shuffle(fluor_arc.T)\n",
    "fluor_arc_2 = fluor_arc_1.T[:,:]\n",
    "\n",
    "fluor_normal_1 = data_shuffle(fluor_normal.T)\n",
    "fluor_normal_2 = fluor_normal_1.T[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Heat dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total re-arranged one-period data shape:  (18000, 2240)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1080)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 7320) \n",
      "\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 4000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Heat arc raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Heat/arc/Heat_1_arc_humid_40(c56)_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Heat/arc/Heat_1_arc_humid_50_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Heat/arc/Heat_2_arc_humid_40_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Heat/arc/Heater_2_arc_humid_34(c27)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4]\n",
    "heat_arc = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "##### Heat normal raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Heat/normal/Heat_1_normal_humid_room_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Heat/normal/Heat_2_normal_humid_40_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2]\n",
    "heat_normal = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "### Heat Data Shuffle\n",
    "heat_arc_1 = data_shuffle(heat_arc.T)\n",
    "heat_arc_2 = heat_arc_1.T[:,:]\n",
    "\n",
    "heat_normal_1 = data_shuffle(heat_normal.T)\n",
    "heat_normal_2 = heat_normal_1.T[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Incan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total re-arranged one-period data shape:  (18000, 1880)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2400)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2400)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1640)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2400)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 880)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2120)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 15720) \n",
      "\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 4000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Incan arc raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"incan/arc/incan_1_arc_humid_room_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"incan/arc/incan_2_arc_44_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_40(c60)_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_40(c61-c120)_data.xlsx\")\n",
    "file_path5 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_40(c121-c161)_data.xlsx\")\n",
    "file_path6 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_95(c60)_data.xlsx\")\n",
    "file_path7 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_95(c61-c82)_data.xlsx\")\n",
    "file_path8 = os.path.join(base_dir, \"incan/arc/Incan_2_arc_humid_room(after humid95)(c53)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4, file_path5, file_path6,\n",
    "             file_path7, file_path8]\n",
    "incan_arc = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "##### Incan normal raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"incan/normal/incan_1_normal_humid_room_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"incan/normal/incan_2_normal_humid_room_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2]\n",
    "incan_normal = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "### Incan Data Shuffle\n",
    "incan_arc_1 = data_shuffle(incan_arc.T)\n",
    "incan_arc_2 = incan_arc_1.T[:,:]\n",
    "\n",
    "incan_normal_1 = data_shuffle(incan_normal.T)\n",
    "incan_normal_2 = incan_normal_1.T[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Led dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total re-arranged one-period data shape:  (18000, 1160)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 760)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1360)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1600)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1800)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1920)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 8600) \n",
      "\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1440)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 2000)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1280)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 1600)\n",
      "[Info.] Total re-arranged one-period data shape:  (18000, 120)\n",
      "[Info.] ==========> Total merged one-period data shape:  (18000, 6440) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### LED arc raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Led/arc/led_1_arc_humid_23_c(30)_data(c29).xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Led/arc/LED_1_arc(c19)_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Led/arc/LED_2_arc(c34)_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Led/arc/led_1_arc(c43)_data.xlsx\")\n",
    "file_path5 = os.path.join(base_dir, \"Led/arc/led_1_arc(c50)_data.xlsx\")\n",
    "file_path6 = os.path.join(base_dir, \"Led/arc/led_2_arc(c56)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4, file_path5, file_path6]\n",
    "led_arc = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "##### LED normal raw dataset\n",
    "file_path1 = os.path.join(base_dir, \"Led/normal/led_1_normal_c(36)_data.xlsx\")\n",
    "file_path2 = os.path.join(base_dir, \"Led/normal/led_1_normal_humid_room_data.xlsx\")\n",
    "file_path3 = os.path.join(base_dir, \"Led/normal/led_2_normal_c(32)_data.xlsx\")\n",
    "file_path4 = os.path.join(base_dir, \"Led/normal/LED_2_normal(c40)_data.xlsx\")\n",
    "file_path5 = os.path.join(base_dir, \"Led/normal/led_1_normal(c3)_data.xlsx\")\n",
    "file_paths = [file_path1, file_path2, file_path3, file_path4, file_path5]\n",
    "led_normal = merge_one_period_data(winT, *file_paths)\n",
    "\n",
    "### LED Data Shuffle\n",
    "led_arc_1 = data_shuffle(led_arc.T)\n",
    "led_arc_2 = led_arc_1.T[:,:]\n",
    "\n",
    "led_normal_1 = data_shuffle(led_normal.T)\n",
    "led_normal_2 = led_normal_1.T[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fan------\n",
      "(18000, 7600)\n",
      "(18000, 8160)\n",
      "(18000, 7600)\n",
      "(18000, 7600)\n",
      "------Fluor------\n",
      "(18000, 7920)\n",
      "(18000, 5480)\n",
      "(18000, 5480)\n",
      "(18000, 5480)\n",
      "------Heat------\n",
      "(18000, 7320)\n",
      "(18000, 4000)\n",
      "(18000, 4000)\n",
      "(18000, 4000)\n",
      "------Incan------\n",
      "(18000, 15720)\n",
      "(18000, 4000)\n",
      "(18000, 4000)\n",
      "(18000, 4000)\n",
      "------Led------\n",
      "(18000, 8600)\n",
      "(18000, 6440)\n",
      "(18000, 6440)\n",
      "(18000, 6440)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불균형으로 데이터를 자름 (oversampling 또는 다른 방법 필요)\n",
    "print('------Fan------')\n",
    "print(fan_arc_2.shape)\n",
    "print(fan_normal_2.shape)\n",
    "fan_normal_2 = fan_normal_1.T[:,:7600]\n",
    "print(fan_arc_2.shape)\n",
    "print(fan_normal_2.shape)\n",
    "\n",
    "print('------Fluor------')\n",
    "print(fluor_arc_2.shape)\n",
    "print(fluor_normal_2.shape)\n",
    "fluor_arc_2 = fluor_arc_1.T[:,:5480]\n",
    "print(fluor_arc_2.shape)\n",
    "print(fluor_normal_2.shape)\n",
    "\n",
    "print('------Heat------')\n",
    "print(heat_arc_2.shape)\n",
    "print(heat_normal_2.shape)\n",
    "heat_arc_2 = heat_arc_1.T[:,:4000]\n",
    "print(heat_arc_2.shape)\n",
    "print(heat_normal_2.shape)\n",
    "\n",
    "print('------Incan------')\n",
    "print(incan_arc_2.shape)\n",
    "print(incan_normal_2.shape)\n",
    "incan_arc_2 = incan_arc_1.T[:,:4000]\n",
    "print(incan_arc_2.shape)\n",
    "print(incan_normal_2.shape)\n",
    "\n",
    "print('------Led------')\n",
    "print(led_arc_2.shape)\n",
    "print(led_normal_2.shape)\n",
    "led_arc_2 = led_arc_1.T[:,:6440]\n",
    "print(led_arc_2.shape)\n",
    "print(led_normal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성추출을 위한 함수 작성 ( 변수이름, 데이터 크기)\n",
    "def create_feature(x, y):\n",
    "    feature = np.zeros(shape=(39,y))\n",
    "    mean_val = np.zeros((10, y))\n",
    "    std_val = np.zeros((10, y))\n",
    "    Pole_diff = np.zeros((10, y))\n",
    "    Avg_diff = np.zeros((9, y))\n",
    "\n",
    "    for i in range(y):\n",
    "        sig = x[:, i]\n",
    "        win = round(x.shape[0]/10)\n",
    "        for j in range(10):\n",
    "            tmp = x[int(j * win) : int((j + 1) * win)]\n",
    "            mean_val[j] = np.mean(tmp[j],axis=0)\n",
    "            std_val[j] = np.std(tmp[j])\n",
    "            Pole_diff[j] = np.max(tmp[j]) - np.min(tmp[j])\n",
    "        for z in range(9):\n",
    "            Avg_diff[z] = np.mean(tmp[z], axis=0) - np.mean(tmp[z+1],axis=0)\n",
    "        \n",
    "    feature = np.concatenate((mean_val, std_val, Pole_diff, Avg_diff), axis =0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5종류 Arc / Normal 특성추출\n",
    "fan_arc = create_feature(fan_arc_2, 7600)\n",
    "fan_normal = create_feature(fan_normal_2, 7600)\n",
    "fluor_arc = create_feature(fluor_arc_2, 5480)\n",
    "fluor_normal = create_feature(fluor_normal_2, 5480)\n",
    "heat_arc = create_feature(heat_arc_2, 4000)\n",
    "heat_normal = create_feature(heat_normal_2, 4000)\n",
    "incan_arc = create_feature(incan_arc_2, 4000)\n",
    "incan_normal = create_feature(incan_normal_2, 4000)\n",
    "led_arc = create_feature(led_arc_2, 6440)\n",
    "led_normal = create_feature(led_normal_2, 6440)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fan------\n",
      "(39, 7600)\n",
      "(39, 7600)\n",
      "------Fluor------\n",
      "(39, 5480)\n",
      "(39, 5480)\n",
      "------Heat------\n",
      "(39, 4000)\n",
      "(39, 4000)\n",
      "------Incan------\n",
      "(39, 4000)\n",
      "(39, 4000)\n",
      "------Led------\n",
      "(39, 6440)\n",
      "(39, 6440)\n"
     ]
    }
   ],
   "source": [
    "# 특성추출한 데이터 크기 확인\n",
    "print('------Fan------')\n",
    "print(fan_arc.shape)\n",
    "print(fan_normal.shape)\n",
    "\n",
    "print('------Fluor------')\n",
    "print(fluor_arc.shape)\n",
    "print(fluor_normal.shape)\n",
    "\n",
    "print('------Heat------')\n",
    "print(heat_arc.shape)\n",
    "print(heat_normal.shape)\n",
    "\n",
    "\n",
    "print('------Incan------')\n",
    "print(incan_arc.shape)\n",
    "print(incan_normal.shape)\n",
    "\n",
    "print('------Led------')\n",
    "print(led_arc.shape)\n",
    "print(led_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Fan Labeling\n",
    "norm_fan_label = np.zeros(fan_normal.shape[1]) # label 0\n",
    "arc_fan_label = np.ones(fan_arc.shape[1])  # label 1\n",
    "\n",
    "##### Fluor Labeling\n",
    "norm_fluor_label = 2*np.ones(fluor_normal.shape[1]) # label 2\n",
    "arc_fluor_label = 3*np.ones(fluor_arc.shape[1])  # label 3\n",
    "\n",
    "##### Heat Labeling\n",
    "norm_heat_label = 4*np.ones(heat_normal.shape[1]) # label 4\n",
    "arc_heat_label = 5*np.ones(heat_arc.shape[1])  # label 5\n",
    "\n",
    "##### Incan Labeling\n",
    "norm_incan_label = 6*np.ones(incan_normal.shape[1]) # label 6\n",
    "arc_incan_label = 7*np.ones(incan_arc.shape[1])  # label 7\n",
    "\n",
    "##### LED Labeling\n",
    "norm_led_label = 8*np.ones(led_normal.shape[1]) # label 8\n",
    "arc_led_label = 9*np.ones(led_arc.shape[1])  # label 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Total dataset shape:  (55040, 39)\n"
     ]
    }
   ],
   "source": [
    "##### Prepare full datasets for training\n",
    "Train_raw = np.concatenate((fan_normal.T, fan_arc.T, fluor_normal.T, fluor_arc.T,\n",
    "                           heat_normal.T, heat_arc.T, incan_normal.T, incan_arc.T, \n",
    "                           led_normal.T, led_arc.T), axis=0)\n",
    "Train_label = np.concatenate((norm_fan_label, arc_fan_label, norm_fluor_label, arc_fluor_label,\n",
    "                             norm_heat_label, arc_heat_label, norm_incan_label, arc_incan_label,\n",
    "                             norm_led_label, arc_led_label))\n",
    "print('[Info.] Total dataset shape: ', Train_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Display label numbers after data shuffling: \n",
      " [0. 0. 5. 0. 3. 1. 1. 7. 3. 7. 0. 6. 0. 1. 2. 3. 7. 4. 2. 7. 7. 2. 1. 6.\n",
      " 0. 8. 3. 5. 5. 0. 1. 2. 6. 9. 1. 1. 3. 3. 0. 3. 3. 6. 7. 3. 8. 0. 2. 1.\n",
      " 3. 6. 3. 0. 1. 7. 7. 2. 2. 3. 8. 9. 8. 2. 6. 9. 9. 8. 9. 0. 1. 2. 7. 6.\n",
      " 9. 0. 9. 6. 2. 8. 3. 4. 1. 7. 4. 6. 0. 8. 9. 4. 7. 5. 7. 2. 8. 2. 0. 1.\n",
      " 9. 8. 8. 4.]\n"
     ]
    }
   ],
   "source": [
    "# s라는 배열에 data의 인덱스를 넣고 섞은 뒤 data와 data_label 배열에 대입.(train)\n",
    "s = np.arange(Train_raw.shape[0])\n",
    "np.random.shuffle(s)\n",
    "\n",
    "data_tr = Train_raw[s]\n",
    "data_tr_label = Train_label[s]\n",
    "print(\"===> Display label numbers after data shuffling:\", \"\\n\", data_tr_label[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Train and Testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] Number of training dataset:  44032\n",
      "[Info.] Number of test dataset:  11008\n",
      "(44032, 39)\n",
      "(11008, 39)\n",
      "(44032,)\n",
      "(11008,)\n"
     ]
    }
   ],
   "source": [
    "SPL_RATIO = 0.8 \n",
    "index = round(data_tr.shape[0]*SPL_RATIO)\n",
    "\n",
    "### train 데이터와 test 데이터로 분리\n",
    "train_X, train_Y = data_tr[:index], data_tr_label[:index]\n",
    "test_X, test_Y = data_tr[index:], data_tr_label[index:]\n",
    "print('[Info.] Number of training dataset: ', len(train_X))\n",
    "print('[Info.] Number of test dataset: ', len(test_X))\n",
    "\n",
    "### Save trainind and test datasets and labels\n",
    "dataset_dir = './TrTedata'\n",
    "if os.path.exists(dataset_dir):  # 반복적인 실행을 위해 디렉토리를 삭제\n",
    "    shutil.rmtree(dataset_dir)   \n",
    "os.mkdir(dataset_dir)\n",
    "\n",
    "# training\n",
    "train_X_dir = os.path.join(dataset_dir, 'train_X.npy')\n",
    "np.save(train_X_dir, train_X)\n",
    "\n",
    "train_Y_dir = os.path.join(dataset_dir, 'train_Y.npy')\n",
    "np.save(train_Y_dir, train_Y)\n",
    "\n",
    "# test\n",
    "test_X_dir = os.path.join(dataset_dir, 'test_X.npy')\n",
    "np.save(test_X_dir, test_X)\n",
    "\n",
    "test_Y_dir = os.path.join(dataset_dir, 'test_Y.npy')\n",
    "np.save(test_Y_dir, test_Y)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info.] x_train numpy shape:  (44032, 39, 1)\n",
      "[Info.] x_test numpy shape:  (11008, 39, 1)\n",
      "[Info.] y_train numpy shape:  (44032,)\n",
      "[Info.] y_test numpy shape:  (11008,)\n"
     ]
    }
   ],
   "source": [
    "### Training and test datasets\n",
    "x_train = np.reshape(train_X, (len(train_X),train_X.shape[1],1))\n",
    "x_test = np.reshape(test_X, (len(test_X),train_X.shape[1],1))\n",
    "\n",
    "### Training and test labeling\n",
    "y_train = np.asarray(train_Y).astype('float32')\n",
    "y_test = np.asarray(test_Y).astype('float32')\n",
    "\n",
    "print('[Info.] x_train numpy shape: ', x_train.shape)\n",
    "print('[Info.] x_test numpy shape: ', x_test.shape)\n",
    "print('[Info.] y_train numpy shape: ', y_train.shape)\n",
    "print('[Info.] y_test numpy shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44032, 39)\n",
      "(11008, 39)\n",
      "(44032,)\n",
      "(11008,)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape((44032, 39))\n",
    "x_test = x_test.reshape((11008, 39))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앙상블 학습의 대표 중 하나로 안정적인 성능 덕분에 널리 사용.\n",
    "- 각 트리를 훈련하기 위한 데이터를 랜덤하게 만든다.\n",
    "- 이 데이터를 만드는 방법은 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다. 이때 한 샘플이 중복되어 추출.(부트스트랩 샘플)\n",
    "- 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾음.\n",
    "- RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택.\n",
    "- 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻음.\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터로 측정한 정확도 = 1.00\n",
      "시험용 데이터로 측정한 정확도 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 정확도 계산.\n",
    "print('학습용 데이터로 측정한 정확도 = %.2f' % rf.score(x_train, y_train))\n",
    "print('시험용 데이터로 측정한 정확도 = %.2f' % rf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트에 대한 예측값:\n",
      " [1. 8. 2. ... 3. 3. 0.]\n",
      "테스트 세트의 정확도: 1.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "print(\"테스트 세트에 대한 예측값:\\n {}\".format(y_pred))\n",
    "print(\"테스트 세트의 정확도: {:.2f}\".format(np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=0.853 total time=   0.7s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=0.781 total time=   0.4s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.5s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=0.853 total time=   0.2s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=0.781 total time=   0.2s\n",
      "[CV 1/2] END max_depth=6, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=6, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END max_depth=8, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=8, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.1s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.1s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.1s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=10, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=10, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=8, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.1s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.1s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=12, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=8, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=8, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=16, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=16, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=20, n_estimators=10;, score=1.000 total time=   0.2s\n",
      "[CV 1/2] END max_depth=12, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/2] END max_depth=12, min_samples_leaf=18, min_samples_split=20, n_estimators=100;, score=1.000 total time=   0.4s\n",
      "최적 하이퍼 파라미터:  {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "최고 예측 정확도: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "params = {'n_estimators':[10, 100], \n",
    "         'max_depth': [6, 8, 10, 12],\n",
    "         'min_samples_leaf' : [8, 12, 18],\n",
    "         'min_samples_split': [8, 16, 20]\n",
    "         }\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs= -1)\n",
    "gs = GridSearchCV(rf, params, cv= KFold(n_splits=2, shuffle=True), scoring='accuracy', verbose = 3)\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "print(\"최적 하이퍼 파라미터: \", gs.best_params_)\n",
    "print(\"최고 예측 정확도: {:.4f}\".format(gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth = 6, min_samples_leaf = 8, min_samples_split = 8, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=6, min_samples_leaf=8, min_samples_split=8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터로 측정한 정확도 = 1.00\n",
      "시험용 데이터로 측정한 정확도 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 정확도 계산.\n",
    "print('학습용 데이터로 측정한 정확도 = %.2f' % rf.score(x_train, y_train))\n",
    "print('시험용 데이터로 측정한 정확도 = %.2f' % rf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors : 분류 시 고려할 인접 샘플 수\n",
    "# weights(default='uniform') : 'distance'로 설정하면, 분류할 때 인접한 샘플의 거리에 따라 다른 가중치 부여 (가까울수록 큰 가중치)\n",
    "# metric(default='minkowski') : 거리계산의 척도\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1, weights=\"distance\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=1, weights='distance')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트에 대한 예측값:\n",
      " [1. 8. 2. ... 3. 3. 0.]\n",
      "테스트 세트의 정확도: 1.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "print(\"테스트 세트에 대한 예측값:\\n {}\".format(y_pred))\n",
    "print(\"테스트 세트의 정확도: {:.2f}\".format(np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5] END .....................n_neighbors=1;, score=1.000 total time=   1.9s\n",
      "[CV 2/5] END .....................n_neighbors=1;, score=1.000 total time=   1.8s\n",
      "[CV 3/5] END .....................n_neighbors=1;, score=1.000 total time=   1.8s\n",
      "[CV 4/5] END .....................n_neighbors=1;, score=1.000 total time=   1.8s\n",
      "[CV 5/5] END .....................n_neighbors=1;, score=1.000 total time=   1.8s\n",
      "[CV 1/5] END .....................n_neighbors=2;, score=1.000 total time=   2.1s\n",
      "[CV 2/5] END .....................n_neighbors=2;, score=1.000 total time=   2.1s\n",
      "[CV 3/5] END .....................n_neighbors=2;, score=1.000 total time=   2.1s\n",
      "[CV 4/5] END .....................n_neighbors=2;, score=1.000 total time=   2.1s\n",
      "[CV 5/5] END .....................n_neighbors=2;, score=1.000 total time=   2.1s\n",
      "[CV 1/5] END .....................n_neighbors=3;, score=1.000 total time=   2.3s\n",
      "[CV 2/5] END .....................n_neighbors=3;, score=1.000 total time=   2.3s\n",
      "[CV 3/5] END .....................n_neighbors=3;, score=1.000 total time=   2.3s\n",
      "[CV 4/5] END .....................n_neighbors=3;, score=1.000 total time=   2.3s\n",
      "[CV 5/5] END .....................n_neighbors=3;, score=1.000 total time=   2.3s\n",
      "[CV 1/5] END .....................n_neighbors=4;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=4;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=4;, score=1.000 total time=   3.5s\n",
      "[CV 4/5] END .....................n_neighbors=4;, score=1.000 total time=   3.5s\n",
      "[CV 5/5] END .....................n_neighbors=4;, score=1.000 total time=   3.3s\n",
      "[CV 1/5] END .....................n_neighbors=5;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=5;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=5;, score=1.000 total time=   3.5s\n",
      "[CV 4/5] END .....................n_neighbors=5;, score=1.000 total time=   3.5s\n",
      "[CV 5/5] END .....................n_neighbors=5;, score=1.000 total time=   3.3s\n",
      "[CV 1/5] END .....................n_neighbors=6;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=6;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=6;, score=1.000 total time=   3.5s\n",
      "[CV 4/5] END .....................n_neighbors=6;, score=1.000 total time=   3.5s\n",
      "[CV 5/5] END .....................n_neighbors=6;, score=1.000 total time=   3.3s\n",
      "[CV 1/5] END .....................n_neighbors=7;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=7;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=7;, score=1.000 total time=   3.5s\n",
      "[CV 4/5] END .....................n_neighbors=7;, score=1.000 total time=   3.5s\n",
      "[CV 5/5] END .....................n_neighbors=7;, score=1.000 total time=   3.3s\n",
      "[CV 1/5] END .....................n_neighbors=8;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=8;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=8;, score=1.000 total time=   3.5s\n",
      "[CV 4/5] END .....................n_neighbors=8;, score=1.000 total time=   3.5s\n",
      "[CV 5/5] END .....................n_neighbors=8;, score=1.000 total time=   3.3s\n",
      "[CV 1/5] END .....................n_neighbors=9;, score=1.000 total time=   3.2s\n",
      "[CV 2/5] END .....................n_neighbors=9;, score=1.000 total time=   3.2s\n",
      "[CV 3/5] END .....................n_neighbors=9;, score=1.000 total time=   3.6s\n",
      "[CV 4/5] END .....................n_neighbors=9;, score=1.000 total time=   3.6s\n",
      "[CV 5/5] END .....................n_neighbors=9;, score=1.000 total time=   3.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             estimator=KNeighborsClassifier(metric='euclidean', n_neighbors=1,\n",
       "                                            weights='distance'),\n",
       "             param_grid=[{'n_neighbors': range(1, 10)}], scoring='accuracy',\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1, weights=\"distance\", metric=\"euclidean\")\n",
    "param_grid = [{'n_neighbors': range(1,10)}]\n",
    "gs = GridSearchCV(knn, param_grid, cv= KFold(n_splits=5, shuffle=True), scoring='accuracy', verbose = 3)\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:  {'n_neighbors': 1}\n",
      "최고 예측 정확도: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 하이퍼 파라미터: \", gs.best_params_)\n",
    "print(\"최고 예측 정확도: {:.4f}\".format(gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, weights=\"distance\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=1, weights='distance')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터로 측정한 정확도 = 1.00\n",
      "시험용 데이터로 측정한 정확도 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 정확도 계산.\n",
    "print('학습용 데이터로 측정한 정확도 = %.2f' % knn.score(x_train, y_train))\n",
    "print('시험용 데이터로 측정한 정확도 = %.2f' % knn.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤 포레스트와 비슷하게 동작.\n",
    "- 결정 트리가 제공하는 대부분의 매개변수를 지원. 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, x_train, y_train, return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01699946 0.00962231 0.02062749 0.02993479 0.03587629 0.04797844\n",
      " 0.03952264 0.03825744 0.02660077 0.02727088 0.02230311 0.01615028\n",
      " 0.01881748 0.01879772 0.01379352 0.01851568 0.01219435 0.01170963\n",
      " 0.02388932 0.01678792 0.02963995 0.01814423 0.02873368 0.03587623\n",
      " 0.03610251 0.02678946 0.02382473 0.0304536  0.0277167  0.02655013\n",
      " 0.0497368  0.02237522 0.00783151 0.0395558  0.01704862 0.05043943\n",
      " 0.01046386 0.02611774 0.02695028]\n"
     ]
    }
   ],
   "source": [
    "et.fit(x_train, y_train)\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터로 측정한 정확도 = 1.00\n",
      "시험용 데이터로 측정한 정확도 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# 정확도 계산.\n",
    "print('학습용 데이터로 측정한 정확도 = %.2f' % et.score(x_train, y_train))\n",
    "print('시험용 데이터로 측정한 정확도 = %.2f' % et.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
